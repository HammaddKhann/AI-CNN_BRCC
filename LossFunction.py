import numpy as np


class CrossEntropy:
    def __init__(self, eps=1e-8):
        self.eps = eps  # to prevent numerical instability
        self.inputs = None
        self.targets = None

    def forward(self, inputs, targets):
        self.inputs = inputs
        self.targets = targets
        inputs = np.clip(inputs, self.eps, 1 - self.eps)

        loss = -np.sum(targets * np.log(inputs) / len(self.targets))  # compute the loss between the predicted probabilities and the true labels.
        return loss

    def backward(self):
        """
        self.inputs: Represents the predicted probabilities generated by the model.
        self.targets: Represents the true labels.
        self.inputs - self.targets: Computes the difference between the predicted probabilities and the true labels.
         This difference represents the error in the predictions.
        """

        d_dict = {"d_out": self.inputs - self.targets}  # computes the gradient of the loss with respect to the inputs
        return d_dict
